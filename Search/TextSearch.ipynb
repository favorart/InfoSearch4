{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, '../map-red')\n",
    "\n",
    "import copy\n",
    "import codecs\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "\n",
    "import utils\n",
    "import s9_archive\n",
    "# from BlackSearch import BlackSearch, BooleanSearch\n",
    "\n",
    "# import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python TestMarks.py -s 9 -e -l \n",
    "\n",
    "all_index = False\n",
    "if  all_index == True:\n",
    "    ndx_name = '../all_index/povarenok_all_index.txt'\n",
    "    bin_name = '../all_index/povarenok_all_s_backward.bin'\n",
    "    len_name = '../all_index/povarenok_all_dlens.txt'\n",
    "\n",
    "    rnk_name = '../all_index/povarenok_all_ranked.txt'\n",
    "    url_name = 'C:\\\\data\\\\povarenok.ru\\\\all\\\\urls.txt'\n",
    "\n",
    "else:\n",
    "    ndx_name = '../map-red/data/povarenok1000_index.txt'\n",
    "    bin_name = '../map-red/data/povarenok1000s_backward.bin'\n",
    "    len_name = '../map-red/data/povarenok1000_dlens.txt'\n",
    "    rnk_name = '../ranked.txt'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archiver =  s9_archive.Simple9Archiver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expecting object: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-7efe0ef7996e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooleanSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndx_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marchiver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlackSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMyLex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdlen_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-0ac107cb7582>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ndx_name, bin_name, archiver, use_json)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndx_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_json\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mparse_constant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_constant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_pairs_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         **kw)\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parse_constant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \"\"\"\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expecting object: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "bs = BooleanSearch(ndx_name, bin_name, archiver)     \n",
    "br = BlackSearch(bs, lex=utils.MyLex(), dlen_name=len_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextSearch(object):\n",
    "    \"\"\" Пассажный алгоритм:\n",
    "    \n",
    "        Пассаж - фрагмент документа, размера, не превышающего заданный, в котором \n",
    "        встречаются все термы запроса, либо значительная часть термов запроса, \n",
    "        суммарный IDF которых превышает заданное ограничение. Набор нескольких \n",
    "        релевантных вхождений, которые расположены не слишком далеко друг от друга.\n",
    "\n",
    "        Скользящее окно — это массив, каждый элемент которого соответствует слову из запроса.\n",
    "        Двигаясь по релевантным вхождениям мы пытаемся связать их с ячейками скользящего окна.\n",
    "        Каждый матчинг приводит к формированию пассажей.  \n",
    "        \n",
    "        Пассажный ранг документа - ранг лучшего пассажа в документе.\n",
    "        Итоговый ранг: вес(tf-idf) + вес(пассажного алгоритма)   (+ вес(парного tf-idf))\n",
    "    \"\"\"\n",
    "    def __init__(self, br,\n",
    "                 stops_filename='./data/StopWords.txt',\n",
    "                 use_synonyms=False,\n",
    "                 synonyms_filename='./data/RusSyn.txt'):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.min_len_meanful_word = 3\n",
    "        with codecs.open(stops_filename, 'r', encoding='utf-8') as f_stops:\n",
    "            self.stops = set(map(lambda line: line.strip().rstrip('\\n'), f_stops.readlines() ))\n",
    "\n",
    "        self.use_synonyms = use_synonyms\n",
    "        if  use_synonyms:\n",
    "            self.syns = {}\n",
    "            with codecs.open(synonyms_filename, 'r', encoding='utf-8') as f_syn:\n",
    "                for line in f_syn:\n",
    "                    splt = line.rstrip().split(u'|')\n",
    "                    if splt > 1:\n",
    "                        norm = br.lex.norm(splt[0])\n",
    "                        self.syns[norm] = list(set([norm] + splt[1].split(u',')))\n",
    "        self.br = br\n",
    "        self.params = [1.] * 7\n",
    "                      # [6.029999999999999, 2.0699999999999985, 9.0, 1.08, \\\n",
    "                      #  2.0700000000000003, 3.059999999999999, 3.059999999999999]\n",
    "\n",
    "    def filter_by_stops(self, query_norms):\n",
    "        \"\"\" Return two (meanful and another) lists of words \"\"\"\n",
    "        means, stops = [], []\n",
    "        for norm in query_norms:\n",
    "            if (len(norm) >= self.min_len_meanful_word) \\\n",
    "                and (norm not in self.stops):\n",
    "                means.append(norm)\n",
    "            else:\n",
    "                stops.append(norm)\n",
    "        return  (means, stops)\n",
    "\n",
    "    def formulate_request(self, query):\n",
    "        \"\"\" Bool Search string\n",
    "        \n",
    "            Returns uniq_query_norms, query_norms_hashes, request\n",
    "        \"\"\"\n",
    "        # Ordered query\n",
    "        query__n_h = [ self.br.lex.normalize(word) for word in query.lower().split() ]\n",
    "        # unique non-ordered normal forms of word in query\n",
    "        query_norms = list(set(map(itemgetter(0),query__n_h)))\n",
    "\n",
    "        query_means, query_stops = self.filter_by_stops(query_norms)\n",
    "        \n",
    "        query_syns = []\n",
    "        if  self.use_synonyms:\n",
    "            query_syns\n",
    "            for i in xrange(len(query_means)):\n",
    "                norm = query_means[i]\n",
    "                try:\n",
    "                    # Append synonims to request\n",
    "                    query_syns += self.syns[norm]\n",
    "                except: continue\n",
    "\n",
    "        request = query_means + query_stops + query_syns\n",
    "        return (query__n_h, query_means, request)\n",
    "\n",
    "    def sliding_window(self, query__n_h, doc_text):\n",
    "        \"\"\" Скользяцее окно \"\"\"\n",
    "        passages = []\n",
    "        N = len(query__n_h)\n",
    "\n",
    "        passage = []\n",
    "        norms_psg = set()\n",
    "        for norm, posit, hash in doc_text:\n",
    "\n",
    "            if  norm in norms_psg:\n",
    "                for i in xrange(len(passage)):\n",
    "                    if passage[i][0] == norm:\n",
    "                        del passage[i]\n",
    "                        break\n",
    "            else:\n",
    "                norms_psg.add(norm)\n",
    "\n",
    "            if len(passage) == N:\n",
    "                in_psg.remove(passage[0][0])\n",
    "                del passage[0]\n",
    "            \n",
    "            passage.append( (norm, posit, hash) )\n",
    "            passages.append( copy.copy(passage) )\n",
    "        return passages\n",
    "\n",
    "    def passage_rank(self, passage, passage_norms, query_norms, query__n_h, doc_id):\n",
    "        \"\"\" Каждый пассаж численно оценивается по следующим показателям:\n",
    "            •\tПолнота:      %слов из запроса в пассажей, все ли слова представлены\n",
    "            •\tРасстояние    от начала документа\n",
    "            •\tКучность:     как число слов не из пассажа между словами пассажа\n",
    "            •\tСлово-форма:  различие слово-форм в пассаже\n",
    "            •\tПорядок слов: транспозиции\n",
    "            •\ttf-idf        ранг пассажа\n",
    "            Returns the vector of passage characteristics\n",
    "            -->  maximaze to get the best\n",
    "        \"\"\"\n",
    "        # passage item: norm, posit, hash\n",
    "        fullness = float(len(passage_norms)) / len(query_norms)\n",
    "\n",
    "        psg_range = (passage[-1][1] - passage[0][1]) + 1.\n",
    "        psg_range = float(len(passage) if psg_range < len(passage) else psg_range)\n",
    "        compactness = 1. - (psg_range - len(passage)) / psg_range\n",
    "\n",
    "        len_text = self.br.doc_lens[doc_id]\n",
    "        close2start = 1. - float(passage[0][1]) / len_text\n",
    "\n",
    "        words_form = 0.\n",
    "        forms = [ (n,h) for n,p,h in passage ]\n",
    "        for norm,hash in query__n_h:\n",
    "            if (norm,hash) in forms:\n",
    "                forms.remove( (norm,hash) )\n",
    "                words_form += 1.\n",
    "        words_form /= len(query__n_h)\n",
    "\n",
    "        all_trs, eq_trs = 0., 0.\n",
    "        q_transpositions = itertools.combinations(range(len(query__n_h)), 2)\n",
    "        p_transpositions = itertools.combinations(range(len(passage)), 2)\n",
    "        for i,j in q_transpositions:\n",
    "            for p,q in p_transpositions:\n",
    "                if  query__n_h[i][0] == passage[p][0] and \\\n",
    "                    query__n_h[j][0] == passage[q][0]:\n",
    "                    eq_trs += 1.\n",
    "                all_trs += 1.\n",
    "        words_order = eq_trs / all_trs if all_trs else 0.\n",
    "        # --> max\n",
    "        return (fullness, compactness, close2start, words_form, words_order)\n",
    "\n",
    "    def ranking(self, query_norms, query__n_h, query_index, doc_id, tfs, idfs):\n",
    "        \"\"\" \"\"\"\n",
    "        doc_text = []\n",
    "        for word, index in query_index.items():\n",
    "            try:\n",
    "                doc_index = index[\"ids\"].index(doc_id)\n",
    "            except: continue\n",
    "            posits, hashes = self.br.bs.decode_posits_and_hashes_for_doc(index, doc_index)\n",
    "            doc_text += zip([word] * min(len(posits), len(hashes)), posits, hashes)\n",
    "\n",
    "        doc_text.sort(key=itemgetter(1))\n",
    "\n",
    "        doc_passages = self.sliding_window(query__n_h, doc_text)\n",
    "\n",
    "        doc_passages_ranges = []\n",
    "        for i, passage in enumerate(doc_passages):\n",
    "            passage_norms = list(set([ n for n,p,h in passage ]))\n",
    "\n",
    "            doc_score = self.br.ranking(passage_norms, query_index, [doc_id], tfs, idfs)\n",
    "            doc, tf_idf = doc_score[0]\n",
    "\n",
    "            vector = list(self.passage_rank(passage, passage_norms, query_norms, query__n_h, doc_id))\n",
    "            # НОРМИРОВАННЫЕ ВЕЛИЧИНЫ:\n",
    "            #   1. Полнота\n",
    "            #   2. Компактность\n",
    "            #   3. Близость к началу\n",
    "            #   4. Слово-форма\n",
    "            #   5. Порядок слов\n",
    "            #   6. tf-idf пассажа\n",
    "            # Итоговый ранк пассажа, как линейная комбинация\n",
    "            # параметров пассажа с коэффициентами = 1.\n",
    "            # --->> MAXIMIZE\n",
    "            doc_passages_ranges.append( vector + [tf_idf] )\n",
    "            \n",
    "        doc_passages_ranges.sort(key=lambda x: sum(x), reverse=True)\n",
    "        return  doc_passages_ranges[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(self, query, max_n_docs=1000, use_synonyms=False, mark_id=None):\n",
    "    \"\"\" Returns the  doc_scores: (doc_id, total_rank) \"\"\"\n",
    "    query__n_h, query_means, request = self.formulate_request(query)\n",
    "\n",
    "    if 1:\n",
    "        inv_q = self.br.lex.incorrect_keyboard_layout(query)\n",
    "        if  inv_q:\n",
    "            inv_q__n_h, inv_q_means, inv_q_req = self.formulate_request(inv_q)\n",
    "            # Append inversed to request line\n",
    "            query__n_h += inv_q__n_h\n",
    "            query_means += inv_q_means\n",
    "            request += inv_q_req\n",
    "\n",
    "        trs_q = self.br.lex.transliterate(query)\n",
    "        if  trs_q:\n",
    "            trs_q__n_h, trs_q_means, trs_q_req = self.formulate_request(trs_q)\n",
    "            # Append transliterated to request line\n",
    "            query__n_h += trs_q__n_h\n",
    "            query_means += trs_q_means\n",
    "            request += trs_q_req\n",
    "\n",
    "        query_index, indexed_doc_ids = self.br.search(request,\n",
    "                                                      up=[\"ids\", \"lens\", \"posits\", \"hashes\"],\n",
    "                                                      verbose=True)\n",
    "\n",
    "        # query_index  is  { norm: [ \"ids\", \"lens\", \"posits\", \"hashes\" ], ... }\n",
    "        query_norms = query_index.keys()\n",
    "        query__n_h = [ (norm, hash)  for norm, hash in query__n_h  if norm in query_norms ]\n",
    "\n",
    "        # If there are STOPs - OK, if not, it is also OK!\n",
    "        intersect_doc_ids = set(indexed_doc_ids)\n",
    "        for word, index in query_index.items():\n",
    "            if word in query_means:\n",
    "                intersect_doc_ids &= set(index['ids'])\n",
    "        # ?? synonyms\n",
    "        intersect_doc_ids = list(intersect_doc_ids)\n",
    "\n",
    "        tfs, idfs  = self.br.tf_idf(query_index, intersect_doc_ids)\n",
    "        doc_scores = self.br.ranking(query_norms, query_index, intersect_doc_ids, tfs, idfs)\n",
    "\n",
    "        # choose documents to ranking by passage algorithm\n",
    "        doc_bm25_scores = doc_scores[:max_n_docs]\n",
    "\n",
    "        doc_pssg_scores = []\n",
    "        for doc_id, score in doc_bm25_scores:\n",
    "            best_passage_rank = self.ranking(query_norms, query__n_h,\n",
    "                                             query_index, doc_id,\n",
    "                                             tfs, idfs)\n",
    "            doc_pssg_scores.append( (doc_id, best_passage_rank) )\n",
    "\n",
    "        doc_pssg_scores.sort(key=itemgetter(0))\n",
    "        doc_bm25_scores.sort(key=itemgetter(0))\n",
    "\n",
    "        # total rank\n",
    "        doc_scores = map(lambda x, y: (x[0], sum(x[1]) + y[1]), doc_pssg_scores, doc_bm25_scores)\n",
    "        doc_scores.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "        doc_ids =  [ doc_id for doc_id, score in doc_scores ]\n",
    "\n",
    "        # if mark_id:\n",
    "        #     try:\n",
    "        #         real_no = doc_ids.index(mark_id)\n",
    "        #     except: return doc_ids\n",
    "        # \n",
    "        #     print '\\nreal_no', real_no\n",
    "        #     vectors = np.ndarray(doc_pssg_scores)\n",
    "        #     print vectors.shape\n",
    "        # \n",
    "        #     vs = np.ndarray(doc_bm25_scores)[:,1]\n",
    "        #     print vs.shape\n",
    "        #     vectors = np.hstack([vectors,  vs])\n",
    "        # \n",
    "        #     with open(mark_id + 'ranged.txt', 'w') as f:\n",
    "        #         pickle.dump(vectors, f)\n",
    "\n",
    "        # if 0:\n",
    "        #     with open(mark_id + 'ranged.txt', 'r') as f:\n",
    "        #         vectors = pickle.loads(f.read())\n",
    "        # \n",
    "        # \n",
    "        #     # if real_no >= 3:\n",
    "        # \n",
    "        #     # vectors = np.array(np.array([d[1] for d in doc_pssg_scores]))\n",
    "        #     # vectors = np.hstack([vectors, np.array([d[1] for d in doc_bm25_scores]) ])\n",
    "        #     \n",
    "        #     # fullness, compactness, close2start, words_form, words_order, bm25\n",
    "        #     # stats, means = zip(*( (np.partition(vals, k)[k], np.mean(vals)) for vals in vectors[:,1:].T ))\n",
    "        #     \n",
    "        #     means = [ np.mean(vals) for vals in vectors[:,1:].T ]\n",
    "        #     vars  = [  np.var(vals) for vals in vectors[:,1:].T ]\n",
    "        # \n",
    "        #     params = np.array([1.] * 7);\n",
    "        #     for i, coord in enumerate(vectors[real_no,:]):\n",
    "        #         if   means[i] <  coord:\n",
    "        #             params[i] *= vars[i]\n",
    "        #         elif means[i] >  coord:\n",
    "        #             params[i] *= 1. / vars[i]\n",
    "        #      \n",
    "        #     # sttl = doc_pssg_scores[real_no][1] + [ doc_bm25_scores[real_no][1] ]\n",
    "        #     # print sttl\n",
    "        #     # params = [0.01] * 7;\n",
    "        #     # j = np.argmax(sttl)\n",
    "        #     # for k,s in enumerate(sttl):\n",
    "        #     #     if  s > 0.9 or k == j:\n",
    "        #     #         params[k] = 1.\n",
    "        # \n",
    "        #     print j, params\n",
    "        #     self.params = np.array(self.params) + params\n",
    "        # \n",
    "        #         # k  = -5\n",
    "        #         # vectors = np.array(doc_pssg_scores)\n",
    "        #         # vectors = np.hstack([vectors, np.array(doc_bm25_scores_)[:,1] ])\n",
    "        #         # # fullness, compactness, close2start, words_form, words_order, bm25\n",
    "        #         # stats, means = zip(*( (np.partition(vals, k)[k], np.mean(vals)) for vals in vectors[:,1:].T ))\n",
    "        #         # \n",
    "        #         # for i, coord in enumerate(vectors[real_no,:]):\n",
    "        #         #     if  stats[i] <= coord:\n",
    "        #         #         params[i] += means[i]\n",
    "        #         #     else:\n",
    "        #         #         params[i] += 1. / means[i]\n",
    "        #         # \n",
    "        #         # with open('params.txt', 'a') as f:\n",
    "        #         #     print >>f, params\n",
    "        #         # self.params += params\n",
    "        #     # else:\n",
    "        #         # self.parmas += [1.] * len(self.params)\n",
    "        #         # with open('params.txt', 'a') as f:\n",
    "        #         #     print >>f, [1.] * len(self.params)\n",
    "        # # except: pass\n",
    "        #     # self.parmas += [1.] * len(self.params)\n",
    "        #     # with open('params.txt', 'a') as f:\n",
    "        #     #     print >>f, '---';\n",
    "\n",
    "        return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = TextSearch(br)\n",
    "\n",
    "query   = u'пирог с камбалой'  #  u'рецепт блюда из птицы'  #  u'картинки эпел джек'\n",
    "mark_id = 81390                #  193112                    #  193860\n",
    "\n",
    "self = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prnt(x):\n",
    "    print u' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "камбала пирог с\n"
     ]
    }
   ],
   "source": [
    "query__n_h, query_means, request = self.formulate_request(query)\n",
    "\n",
    "inv_q = self.br.lex.incorrect_keyboard_layout(query)\n",
    "if  inv_q:\n",
    "    inv_q__n_h, inv_q_means, inv_q_req = self.formulate_request(inv_q)\n",
    "    # Append inversed to request line\n",
    "    query__n_h += inv_q__n_h\n",
    "    query_means += inv_q_means\n",
    "    request += inv_q_req\n",
    "\n",
    "trs_q = self.br.lex.transliterate(query)\n",
    "if  trs_q:\n",
    "    trs_q__n_h, trs_q_means, trs_q_req = self.formulate_request(trs_q)\n",
    "    # Append transliterated to request line\n",
    "    query__n_h += trs_q__n_h\n",
    "    query_means += trs_q_means\n",
    "    request += trs_q_req\n",
    "    \n",
    "prnt(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_index, indexed_doc_ids = self.br.search(request,\n",
    "                                              up=[\"ids\", \"lens\", \"posits\", \"hashes\"],\n",
    "                                              verbose=True)\n",
    "\n",
    "# query_index  is  { norm: [ \"ids\", \"lens\", \"posits\", \"hashes\" ], ... }\n",
    "query_norms = query_index.keys()\n",
    "query__n_h = [ (norm, hash)  for norm, hash in query__n_h  if norm in query_norms ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с\n",
      "199456 103899\n",
      "п и р о г\n",
      "199456 181331\n",
      "к а м б а л а\n",
      "197436 434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k,v in query_index.items():\n",
    "    prnt(k)\n",
    "    print v['ids'][-1], len(v['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199])\n",
      "[18434, 143362, 14355, 61469, 131102, 12324, 26665, 192554, 137287, 24650, 24651, 51279, 122960, 116820, 28760, 131162, 129117, 112736, 51303, 168042, 129131, 88172, 153715, 192645, 32902, 114824, 33815, 118926, 55442, 168083, 32918, 139415, 166049, 84137, 79218, 49327, 185374, 78019, 168132, 150902, 155848, 15053, 118993, 133341, 143583, 168160, 73953, 49382, 18679, 112894, 100609, 151814, 147728, 20753, 16665, 155945, 141925, 158003, 80187, 74046, 82252, 43344, 192863, 90491, 125310, 24960, 82310, 41361, 152303, 59813, 190895, 436, 16825, 135619, 84420, 169378, 192979, 176607, 14821, 84457, 156139, 152045, 150270, 180739, 152071, 35342, 143887, 156419, 182804, 93614, 43543, 158251, 133896, 72249, 158271, 94790, 117324, 164432, 193116, 92773, 41746, 150132, 4729, 21115, 137324, 164505, 74395, 121501, 184991, 6817, 33442, 39590, 150641, 127657, 113324, 152239, 183411, 98998, 60091, 53952, 115393, 137923, 80582, 185031, 180940, 80589, 135900, 133855, 10980, 88805, 166639, 88872, 33522, 74484, 103160, 29433, 183035, 168702, 56063, 33539, 142088, 11025, 62226, 11038, 176930, 115496, 156460, 41783, 68408, 68409, 33596, 64317, 27455, 78659, 13125, 13132, 144209, 45914, 66395, 39786, 62315, 140140, 52029, 168823, 80764, 119680, 88965, 41868, 181137, 103319, 183194, 131999, 168864, 60327, 156588, 125871, 28147, 191419, 66492, 169157, 56265, 137992, 127954, 78806, 111576, 134112, 27636, 64503, 144387, 1034, 1036, 125968, 136209, 42002, 109591, 195614, 68647, 136237, 169008, 13361, 154683, 109641, 169041, 97362, 120675, 76893, 11364, 148581, 5223, 130153, 21612, 167021, 121967, 134257, 58483, 129556, 99450, 87165, 171137, 68749, 121999, 83089, 102595, 195739, 7324, 136349, 70815, 122022, 29870, 189616, 79030, 5303, 187576, 119996, 36030, 54463, 130242, 124101, 36041, 83151, 1244, 128223, 126177, 187623, 179432, 27882, 36079, 66806, 36087, 25848, 23424, 70922, 83211, 118028, 56589, 154901, 1312, 44321, 101670, 185640, 6023, 46382, 73016, 44347, 68929, 7492, 107845, 151777, 58696, 179531, 136527, 17748, 195934, 126313, 25965, 140654, 5488, 140658, 134518, 169338, 66939, 89470, 34281, 19847, 73113, 36250, 126363, 134042, 185762, 119663, 114094, 87476, 62901, 177737, 26050, 187845, 56775, 7626, 79095, 103889, 142804, 89558, 21979, 144864, 135760, 56809, 161259, 81389, 81390, 30191, 24147, 60916, 73207, 71163, 122385, 128531, 103956, 144917, 62999, 192027, 67101, 79395, 69158, 185897, 181804, 64020, 194096, 30261, 161334, 75359, 20034, 34371, 104008, 64780, 63053, 183891, 18015, 87650, 65144, 106108, 7806, 196224, 75417, 71329, 129219, 9901, 177845, 20150, 24259, 179911, 173773, 132816, 87672, 52949, 44758, 61146, 112363, 18162, 124660, 190197, 151310, 55057, 73491, 141615, 153373, 96030, 132898, 48933, 173863, 89901, 14127, 182075, 147262, 83776, 130890, 149956, 67407, 188241, 188242, 28500, 61283, 151396, 130918, 184179, 32633, 69500, 167807, 169863, 61321, 89999, 8087, 141211, 90012, 80539, 190374, 44969, 171946, 3399, 32695, 66761, 184251, 131015, 196556, 131022, 169328, 20434, 157657, 159714, 128996, 34793, 139242, 192492, 129005, 129014]\n"
     ]
    }
   ],
   "source": [
    "# If there are STOPs - OK, if not, it is also OK!\n",
    "intersect_doc_ids = set(indexed_doc_ids)\n",
    "for word, index in query_index.items():\n",
    "    if word in query_means:\n",
    "        intersect_doc_ids &= set(index['ids'])\n",
    "# ?? synonyms\n",
    "# print intersect_doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfs, idfs  = self.br.tf_idf(query_index, intersect_doc_ids)\n",
    "doc_scores = self.br.ranking(query_norms, query_index, intersect_doc_ids, tfs, idfs)\n",
    "# choose documents to ranking by passage algorithm\n",
    "doc_bm25_scores = doc_scores[:1000]\n",
    "# print doc_bm25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ad09a4734720>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     best_passage_rank = self.ranking(query_norms, query__n_h,\n\u001b[0;32m      4\u001b[0m                                      \u001b[0mquery_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                      tfs, idfs)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdoc_pssg_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_passage_rank\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-24c93a76a368>\u001b[0m in \u001b[0;36mranking\u001b[1;34m(self, query_norms, query__n_h, query_index, doc_id, tfs, idfs)\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mdoc_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mposits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_posits_and_hashes_for_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mdoc_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-0ac107cb7582>\u001b[0m in \u001b[0;36mdecode_posits_and_hashes_for_doc\u001b[1;34m(self, index, doc_index)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0mdoc_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mdecoded_posits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchiver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "doc_pssg_scores = []\n",
    "for doc_id, score in doc_bm25_scores:\n",
    "    best_passage_rank = self.ranking(query_norms, query__n_h,\n",
    "                                     query_index, doc_id,\n",
    "                                     tfs, idfs)\n",
    "    doc_pssg_scores.append( (doc_id, best_passage_rank) )\n",
    "\n",
    "doc_pssg_scores.sort(key=itemgetter(0))\n",
    "doc_bm25_scores.sort(key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148581, 144387, 99450, 130918, 68408, 144917, 168083, 103889, 98998, 134112]\n"
     ]
    }
   ],
   "source": [
    "# total rank\n",
    "doc_scores = map(lambda x, y: (x[0], sum(x[1]) + y[1]), doc_pssg_scores, doc_bm25_scores)\n",
    "doc_scores.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "doc_ids =  [ doc_id for doc_id, score in doc_scores ]\n",
    "print doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if mark_id in doc_ids:\n",
    "    print \"OK\"\n",
    "    print doc_ids.index(mark_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brute_coefs(TextSearch, marks):\n",
    "    \"\"\" Алгоритм  дифференциaльной эволюции (differential evolution) \n",
    "\n",
    "        На каждой итерации генерируется множество векторов, называемых поколением, \n",
    "        случайным образом комбинируя векторы из предыдущего поколения.\n",
    "\n",
    "        Для каждого вектора x_i из старого поколения выбираются три различных случайных вектора\n",
    "        v_1, v_2, v_3 среди векторов старого поколения, за исключением самого вектора x_i, и\n",
    "        генерируется так называемый мутантный вектор (mutant vector) по формуле:\n",
    "\n",
    "        v = v_1 + F \\cdot (v_2 - v_3),\n",
    "\n",
    "        где F — один из параметров метода, некоторая положительная действительная константа в интервале [0, 2].\n",
    "\n",
    "        Над мутантным вектором v выполняется операция «скрещивания» (crossover), состоящая в том, что некоторые\n",
    "        его координаты замещаются соответствующими координатами из исходного вектора x_i (каждая координата\n",
    "        замещается с некоторой вероятностью, которая также является еще одним из параметров этого метода).\n",
    "        Полученный после скрещивания вектор называется пробным вектором (trial vector). \n",
    "        Если он оказывается лучше вектора x_i (то есть значение целевой функции стало меньше),\n",
    "        то в новом поколении вектор x_i заменяется на пробный вектор, а в противном случае — остаётся x_i.\n",
    "    \"\"\"\n",
    "\n",
    "    # fit \n",
    "    self.params\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pair_tf_idf(TextSearch):\n",
    "    \"\"\" Вхождение терма в документ учитывается только в \n",
    "        том случае, если оно находится в документе на \n",
    "        расстоянии, не превышающим заданное от хотя бы \n",
    "        одного из стоящих рядом с ним термов запроса.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
